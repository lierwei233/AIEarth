{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "protected-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import shutil\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-chick",
   "metadata": {},
   "source": [
    "# 数据与模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deluxe-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定上传或者本地跑模型\n",
    "UPLOAD = False\n",
    "if UPLOAD:\n",
    "    data_train_dir = '/tcdata/enso_round1_train_20210201/'\n",
    "    data_test_dir = '/tcdata/enso_final_test_data_B/'\n",
    "    check_point_path = '/check_point/checkpoint.pt'\n",
    "else:\n",
    "    data_train_dir = '/data/anonym5/zrk/AIEarth/train/'\n",
    "    data_test_dir = '/data/anonym5/zrk/AIEarth/test/'\n",
    "    check_point_path = '/data/anonym5/zrk/AIEarth/check_point/checkpoint.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "banned-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 427):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(20210330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "breathing-brass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4645, Valid samples: 100\n"
     ]
    }
   ],
   "source": [
    "class EarthDataSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['sst'])\n",
    "\n",
    "    def __getitem__(self, idx):   \n",
    "        return (self.data['sst'][idx], self.data['t300'][idx], self.data['ua'][idx], self.data['va'][idx]), self.data['label'][idx]\n",
    "\n",
    "    \n",
    "def load_data2():\n",
    "    # CMIP data    \n",
    "    train = xr.open_dataset(data_train_dir + 'CMIP_train.nc')\n",
    "    label = xr.open_dataset(data_train_dir + 'CMIP_label.nc')\n",
    "   \n",
    "    train_sst = train['sst'][:, :12].values  # (4645, 12, 24, 72)\n",
    "    train_t300 = train['t300'][:, :12].values\n",
    "    train_ua = train['ua'][:, :12].values\n",
    "    train_va = train['va'][:, :12].values\n",
    "    train_label = label['nino'][:, :].values\n",
    "\n",
    "    train_ua = np.nan_to_num(train_ua)\n",
    "    train_va = np.nan_to_num(train_va)\n",
    "    train_t300 = np.nan_to_num(train_t300)\n",
    "    train_sst = np.nan_to_num(train_sst)\n",
    "\n",
    "    # SODA data    \n",
    "    train2 = xr.open_dataset(data_train_dir + 'SODA_train.nc')\n",
    "    label2 = xr.open_dataset(data_train_dir + 'SODA_label.nc')\n",
    "    \n",
    "    train_sst2 = train2['sst'][:, :12].values  # (4645, 12, 24, 72)\n",
    "    train_t3002 = train2['t300'][:, :12].values\n",
    "    train_ua2 = train2['ua'][:, :12].values\n",
    "    train_va2 = train2['va'][:, :12].values\n",
    "    train_label2 = label2['nino'][:, :].values\n",
    "\n",
    "    print('Train samples: {}, Valid samples: {}'.format(len(train_label), len(train_label2)))\n",
    "    \n",
    "    dict_train = {\n",
    "        'sst':train_sst,\n",
    "        't300':train_t300,\n",
    "        'ua':train_ua,\n",
    "        'va': train_va,\n",
    "        'label': train_label}\n",
    "    dict_valid = {\n",
    "        'sst':train_sst2,\n",
    "        't300':train_t3002,\n",
    "        'ua':train_ua2,\n",
    "        'va': train_va2,\n",
    "        'label': train_label2}\n",
    "    train_dataset = EarthDataSet(dict_train)\n",
    "    valid_dataset = EarthDataSet(dict_valid)\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "\n",
    "fit_params = {\n",
    "    'n_epochs' : 5000,\n",
    "    'learning_rate' : 8e-6,\n",
    "    'batch_size' : 64,\n",
    "}\n",
    "\n",
    "train_dataset, valid_dataset = load_data2()\n",
    "train_loader = DataLoader(train_dataset, batch_size=fit_params['batch_size'])\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=fit_params['batch_size'])\n",
    "refine_loader = DataLoader(valid_dataset,batch_size=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-shakespeare",
   "metadata": {},
   "source": [
    "# 模型定义\n",
    "\n",
    "**gz 3月29日晚修改，主要是基于nature文章**\n",
    "- 在pool2之后新增一个Conv2d层；\n",
    "- 每层的conv后都新增了一个tanh，实现卷积层的非线性化，基于nature文章中公式：\n",
    "\n",
    "  $$\n",
    "  \\mathbf{v}_{i, j}^{x, y}=\\tanh \\left(\\sum_{m=1}^{M_{i-1}} \\sum_{p=1}^{P_{i}} \\sum_{q=1}^{Q_{i}} w_{i, j, m}^{p, q} v_{(i-1), m}^{\\left(x+p-P_{i} / 2, y+q-Q_{i} / 2\\right)}+b_{i, j}\\right)\n",
    "  $$\n",
    "- 去掉batchnorm\n",
    "- 最后变成两个linear层，隐藏层元素个数变化为$(30 \\times 108) \\rightarrow 50 \\rightarrow 24 $\n",
    "\n",
    "修改后，模型收敛速度变快，效果并没有太大的提升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "heard-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleSpatailTimeNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleSpatailTimeNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=6, out_channels=30, kernel_size=(4, 8)) # [batch, 30, 24, 72]\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=30, out_channels=30, kernel_size=(2, 4))\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(2,2))\n",
    "        self.conv3 = nn.Conv2d(in_channels=30, out_channels=30, kernel_size=(2, 4))\n",
    "#         self.batch_norm = nn.BatchNorm1d(30, affine=False)\n",
    "#         lstm = nn.LSTM(108, 8, 2, bidirectional=True, batch_first=True)\n",
    "#         self.pool3 = nn.AdaptiveAvgPool2d((1, 108))\n",
    "#         lstm = nn.LSTM(32, 1, 2, bidirectional=True, batch_first=True)\n",
    "#         self.linear = nn.Linear(108, 24)\n",
    "        self.linear1 = nn.Linear(30*108, 50)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(50, 24)\n",
    "    \n",
    "    def forward(self, sst, t300):\n",
    "        \"\"\"\n",
    "        layer 1 (padding + conv1)             [b, 6, 24, 72] --> [b, 30, 24, 72]\n",
    "        layer 2 (pool1)                       [b, 30, 24, 72] --> [b, 30, 12, 36]\n",
    "        layer 3 (padding + conv2)             [b, 30, 12, 36] --> [b, 30, 12, 36]\n",
    "        layer 4 (pool2)                       [b, 30, 12, 36] --> [b, 30, 6,  18]\n",
    "        layer 5 (padding + conv3)             [b, 30, 6,  18] --> [b, 30, 6,  18]\n",
    "        layer 6 (flatten + linear + tanh)     [b, 30*6*18]     --> [b, 50]\n",
    "        layer 7 (flatten + linear + tanh)     [b, 50]      --> [b, 24]\n",
    "        \"\"\"\n",
    "        inp = torch.cat([sst, t300], dim=1)\n",
    "#         print(inp.shape)\n",
    "    \n",
    "        # 1: padding + conv1:\n",
    "        out1 = F.pad(inp, [4, 3, 2, 1])  # 先padding，后卷积\n",
    "        out1 = self.conv1(out1)\n",
    "        out1 = self.tanh(out1)\n",
    "        \n",
    "        # 2: pool1:\n",
    "        out2 = self.pool1(out1)\n",
    "        \n",
    "        # 3: padding + conv2:\n",
    "        out3 = F.pad(out2, [2, 1, 1, 0])\n",
    "        out3 = self.conv2(out3)\n",
    "        out3 = self.tanh(out3)\n",
    "        \n",
    "        # 4: pool2:\n",
    "        out4 = self.pool2(out3)\n",
    "        \n",
    "        # 5: padding + conv3:\n",
    "        out5 = F.pad(out4, [2, 1, 1, 0])\n",
    "        out5 = self.conv3(out5)\n",
    "        out5 = self.tanh(out5)\n",
    "        \n",
    "        # 6: flatten + linear + tanh\n",
    "        out6 = torch.flatten(out5, start_dim=1)\n",
    "        out6 = self.linear1(out6)\n",
    "        out6 = self.tanh(out6)\n",
    "        \n",
    "        # 7: linear\n",
    "        out7 = self.linear2(out6)\n",
    "    \n",
    "    \n",
    "        return out7\n",
    "\n",
    "\n",
    "# model1 = simpleSpatailTimeNN(kernels=kernels)\n",
    "models = []\n",
    "for i in range(10):\n",
    "    models.append(simpleSpatailTimeNN())\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'   \n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=fit_params['learning_rate'])\n",
    "# loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-style",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "threaded-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coreff(x, y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    c1 = sum((x - x_mean) * (y - y_mean))\n",
    "    c2 = sum((x - x_mean)**2) * sum((y - y_mean)**2)\n",
    "    return c1/np.sqrt(c2)\n",
    "\n",
    "def rmse(preds, y):\n",
    "    return np.sqrt(sum((preds - y)**2)/preds.shape[0])\n",
    "\n",
    "def eval_score(preds, label):\n",
    "    # preds = preds.cpu().detach().numpy().squeeze()\n",
    "    # label = label.cpu().detach().numpy().squeeze()\n",
    "    acskill = 0\n",
    "    RMSE = 0\n",
    "    a = 0\n",
    "    a = [1.5]*4 + [2]*7 + [3]*7 + [4]*6\n",
    "    for i in range(24):\n",
    "        RMSE += rmse(label[:, i], preds[:, i])\n",
    "        cor = coreff(label[:, i], preds[:, i])\n",
    "    \n",
    "        acskill += a[i] * np.log(i+1) * cor\n",
    "    return 2/3 * acskill - RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-breach",
   "metadata": {},
   "source": [
    "从github上借鉴的更高级的EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sought-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_score_max = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(score, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, score, model):\n",
    "        '''Saves model when validation score increases.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation score increased ({self.val_score_max:.6f} --> {score:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_score_max = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-mileage",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score increased (inf --> -10.657332).  Saving model ...\n",
      "Validation score increased (-10.657332 --> -6.978342).  Saving model ...\n",
      "Validation score increased (-6.978342 --> -5.758021).  Saving model ...\n",
      "Validation score increased (-5.758021 --> -4.788464).  Saving model ...\n",
      "Validation score increased (-4.788464 --> -3.676661).  Saving model ...\n",
      "Validation score increased (-3.676661 --> -2.189165).  Saving model ...\n",
      "Validation score increased (-2.189165 --> -0.427866).  Saving model ...\n",
      "Validation score increased (-0.427866 --> 1.309987).  Saving model ...\n",
      "Validation score increased (1.309987 --> 2.854750).  Saving model ...\n",
      "Validation score increased (2.854750 --> 4.162124).  Saving model ...\n",
      "Validation score increased (4.162124 --> 5.245639).  Saving model ...\n",
      "Validation score increased (5.245639 --> 6.135873).  Saving model ...\n",
      "Validation score increased (6.135873 --> 6.863151).  Saving model ...\n",
      "Validation score increased (6.863151 --> 7.452522).  Saving model ...\n",
      "Validation score increased (7.452522 --> 7.923754).  Saving model ...\n",
      "Validation score increased (7.923754 --> 8.292635).  Saving model ...\n",
      "Validation score increased (8.292635 --> 8.572237).  Saving model ...\n",
      "Validation score increased (8.572237 --> 8.773800).  Saving model ...\n",
      "Validation score increased (8.773800 --> 8.907315).  Saving model ...\n",
      "Validation score increased (8.907315 --> 8.981872).  Saving model ...\n",
      "Validation score increased (8.981872 --> 9.005865).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 20\n",
      "EarlyStopping counter: 2 out of 20\n",
      "EarlyStopping counter: 3 out of 20\n",
      "EarlyStopping counter: 4 out of 20\n",
      "EarlyStopping counter: 5 out of 20\n",
      "EarlyStopping counter: 6 out of 20\n",
      "EarlyStopping counter: 7 out of 20\n",
      "EarlyStopping counter: 8 out of 20\n",
      "EarlyStopping counter: 9 out of 20\n",
      "EarlyStopping counter: 10 out of 20\n",
      "EarlyStopping counter: 11 out of 20\n",
      "EarlyStopping counter: 12 out of 20\n",
      "EarlyStopping counter: 13 out of 20\n",
      "EarlyStopping counter: 14 out of 20\n",
      "EarlyStopping counter: 15 out of 20\n",
      "EarlyStopping counter: 16 out of 20\n",
      "EarlyStopping counter: 17 out of 20\n",
      "EarlyStopping counter: 18 out of 20\n",
      "EarlyStopping counter: 19 out of 20\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Validation score increased (inf --> -15.384881).  Saving model ...\n",
      "Validation score increased (-15.384881 --> -12.234270).  Saving model ...\n",
      "Validation score increased (-12.234270 --> -10.494399).  Saving model ...\n",
      "Validation score increased (-10.494399 --> -8.914546).  Saving model ...\n",
      "Validation score increased (-8.914546 --> -7.192526).  Saving model ...\n",
      "Validation score increased (-7.192526 --> -5.625841).  Saving model ...\n",
      "Validation score increased (-5.625841 --> -3.522939).  Saving model ...\n",
      "Validation score increased (-3.522939 --> -1.341415).  Saving model ...\n",
      "Validation score increased (-1.341415 --> 0.667125).  Saving model ...\n",
      "Validation score increased (0.667125 --> 2.412734).  Saving model ...\n",
      "Validation score increased (2.412734 --> 3.876368).  Saving model ...\n",
      "Validation score increased (3.876368 --> 5.066746).  Saving model ...\n",
      "Validation score increased (5.066746 --> 6.006115).  Saving model ...\n",
      "Validation score increased (6.006115 --> 6.726191).  Saving model ...\n",
      "Validation score increased (6.726191 --> 7.263664).  Saving model ...\n",
      "Validation score increased (7.263664 --> 7.654551).  Saving model ...\n",
      "Validation score increased (7.654551 --> 7.930117).  Saving model ...\n",
      "Validation score increased (7.930117 --> 8.115388).  Saving model ...\n",
      "Validation score increased (8.115388 --> 8.229506).  Saving model ...\n",
      "Validation score increased (8.229506 --> 8.286895).  Saving model ...\n",
      "Validation score increased (8.286895 --> 8.298484).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 20\n",
      "EarlyStopping counter: 2 out of 20\n",
      "EarlyStopping counter: 3 out of 20\n",
      "EarlyStopping counter: 4 out of 20\n",
      "EarlyStopping counter: 5 out of 20\n",
      "EarlyStopping counter: 6 out of 20\n",
      "EarlyStopping counter: 7 out of 20\n",
      "EarlyStopping counter: 8 out of 20\n",
      "EarlyStopping counter: 9 out of 20\n",
      "EarlyStopping counter: 10 out of 20\n",
      "EarlyStopping counter: 11 out of 20\n",
      "EarlyStopping counter: 12 out of 20\n",
      "EarlyStopping counter: 13 out of 20\n",
      "EarlyStopping counter: 14 out of 20\n",
      "EarlyStopping counter: 15 out of 20\n",
      "EarlyStopping counter: 16 out of 20\n",
      "EarlyStopping counter: 17 out of 20\n",
      "EarlyStopping counter: 18 out of 20\n",
      "EarlyStopping counter: 19 out of 20\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Validation score increased (inf --> -13.628027).  Saving model ...\n",
      "Validation score increased (-13.628027 --> -8.255169).  Saving model ...\n",
      "Validation score increased (-8.255169 --> -5.768483).  Saving model ...\n",
      "Validation score increased (-5.768483 --> -5.417967).  Saving model ...\n",
      "Validation score increased (-5.417967 --> -5.080235).  Saving model ...\n",
      "Validation score increased (-5.080235 --> -3.711323).  Saving model ...\n",
      "Validation score increased (-3.711323 --> -1.856508).  Saving model ...\n",
      "Validation score increased (-1.856508 --> -0.072546).  Saving model ...\n",
      "Validation score increased (-0.072546 --> 1.389266).  Saving model ...\n",
      "Validation score increased (1.389266 --> 2.503107).  Saving model ...\n",
      "Validation score increased (2.503107 --> 3.328017).  Saving model ...\n",
      "Validation score increased (3.328017 --> 3.933228).  Saving model ...\n",
      "Validation score increased (3.933228 --> 4.375921).  Saving model ...\n",
      "Validation score increased (4.375921 --> 4.697495).  Saving model ...\n",
      "Validation score increased (4.697495 --> 4.926258).  Saving model ...\n",
      "Validation score increased (4.926258 --> 5.081475).  Saving model ...\n",
      "Validation score increased (5.081475 --> 5.176756).  Saving model ...\n",
      "Validation score increased (5.176756 --> 5.222392).  Saving model ...\n",
      "Validation score increased (5.222392 --> 5.226799).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 20\n",
      "EarlyStopping counter: 2 out of 20\n"
     ]
    }
   ],
   "source": [
    "patience = 20  # 经过20个epoch没有提升后，earlystopping，同时回溯到最佳的epoch\n",
    "\n",
    "Valid_Score = np.zeros([10])\n",
    "for j in range(10):\n",
    "    model = models[j]\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=check_point_path)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=fit_params['learning_rate'])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.to(device)\n",
    "    loss_fn.to(device)\n",
    "\n",
    "    for i in range(fit_params['n_epochs']):\n",
    "        model.train()\n",
    "        for step, ((sst, t300, ua, va), label) in enumerate(train_loader):  \n",
    "            sst = sst[:,j:j+3,:,:]\n",
    "            t300 = t300[:,j:j+3,:,:]\n",
    "            label = label[:,j+3:j+27]\n",
    "#             label = label[:,j+3:j+27]\n",
    "#             print(label.shape)\n",
    "            sst = sst.to(device).float()\n",
    "            t300 = t300.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            label = label.to(device).float()\n",
    "            preds = model(sst, t300)\n",
    "            loss = loss_fn(preds, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    #         print('Step: {}, Train Loss: {}'.format(step, loss))\n",
    "\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        for step, ((sst, t300, ua, va), label) in enumerate(valid_loader):\n",
    "            sst = sst[:,j:j+3,:,:]\n",
    "            t300 = t300[:,j:j+3,:,:]\n",
    "            label = label[:,j+3:j+27]\n",
    "            sst = sst.to(device).float()\n",
    "            t300 = t300.to(device).float()\n",
    "            label = label.to(device).float()\n",
    "            preds = model(sst, t300,)\n",
    "\n",
    "            y_pred.append(preds)\n",
    "            y_true.append(label)\n",
    "\n",
    "        y_true = torch.cat(y_true, dim=0)\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "        score = eval_score(y_true.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n",
    "        early_stopping(score, model)\n",
    "#         if i % 10 == 0:\n",
    "#             print(f'Epoch: {i+1}, Valid Score {score}')\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "            \n",
    "    Valid_Score[j] = early_stopping.best_score\n",
    "    model.load_state_dict(torch.load(check_point_path))  # 回溯到之前保存的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid_Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-arcade",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = np.array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
    "                       10, 10, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1])  # 13~36月中每个月涉及到的模型的个数。最后一个月（36）涉及的模型数为1\n",
    "Valid_Score[Valid_Score < 0] = 0\n",
    "\n",
    "def predict(sst, t300):\n",
    "    # 根据10个模型，(b, 12, 24, 72) * 4 的输入，计算(b, 24)的输出\n",
    "    y_pred = np.zeros(shape=[sst.shape[0], 24])\n",
    "    for i in range(10):\n",
    "        # 第i个模型对应的是(i):(i+3)的输入，(i+3):(i+27)的预测。\n",
    "        model = models[i]\n",
    "        model.eval()\n",
    "        y_pred_temp = model(sst[:, i:i+3, :, :], t300[:, i:i+3, :, :])\n",
    "        y_pred_temp = y_pred_temp.detach().cpu().numpy()\n",
    "        start = 12  # 有用预测的开始月\n",
    "        end = i + 27  # 有用预测的结束月\n",
    "        # 因为y_pred中是从第13月开始的，所以start和end要再减12\n",
    "        start = start - 12\n",
    "        end = end - 12\n",
    "        num_valid_month = end - start  # 真正有用的预测月份个数\n",
    "        \n",
    "        # 权重\n",
    "        weights = np.zeros([1, num_valid_month])\n",
    "        for j in range(num_valid_month):\n",
    "            num_model = num_models[j]  # 预测第12+j月的有num_model个模型\n",
    "            weights[0, j] = Valid_Score[i] / Valid_Score[-num_model:].sum()\n",
    "#             if j == num_valid_month - 1:\n",
    "#                 print(weights)\n",
    "        y_pred[:, :num_valid_month] = y_pred[:, :num_valid_month] + weights * y_pred_temp[:, start:end]\n",
    "    return y_pred\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "for step, ((sst, t300, ua, va), label) in enumerate(valid_loader):\n",
    "    sst = sst[:, :, :, :].to(device).float()\n",
    "    t300 = t300[:,:,:,:].to(device).float()\n",
    "#     ua = ua[:,:,:,:].to(device).float()\n",
    "#     va = va[:,:,:,:].to(device).float()\n",
    "    \n",
    "    label = label[:, 12:].cpu().numpy()\n",
    "    preds = predict(sst, t300)\n",
    "\n",
    "    y_pred.append(preds)\n",
    "    y_true.append(label)\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "print(\"Final score on SODA:\", eval_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "teX = np.load('/data/anonym5/zrk/AIEarth/test/test_0144-01-12.npy','r')\n",
    "teY = np.load('/data/anonym5/zrk/AIEarth/test/label/test_0144-01-12.npy','r')\n",
    "for j in range(10):\n",
    "    model = models[j]\n",
    "    te_sst = torch.from_numpy(teX[:, :, :, 0].reshape(1, 12, 24, 72)).to(device).float()\n",
    "    te_t300 = torch.from_numpy(teX[:, :, :, 1].reshape(1, 12, 24, 72)).to(device).float()\n",
    "#     te_ua = torch.from_numpy(teX[:, :, :, 2].reshape(1, 12, 24, 72)).to(device).float()\n",
    "#     te_va = torch.from_numpy(teX[:, :, :, 3].reshape(1, 12, 24, 72)).to(device).float()\n",
    "\n",
    "    te_pred = predict(te_sst, te_t300)\n",
    "\n",
    "    te_pred = te_pred[0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(te_pred)\n",
    "plt.plot(teY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-withdrawal",
   "metadata": {},
   "source": [
    "# 上传\n",
    "\n",
    "如果不上传docker，不要运行下面的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPLOAD:\n",
    "    res_dir = '/result/'\n",
    "\n",
    "    # predict and save to './result/' directory\n",
    "    for filename in os.listdir(data_test_dir):\n",
    "        teX = np.load(data_test_dir + filename)\n",
    "        te_sst = torch.from_numpy(teX[:, :, :, 0].reshape(1, 12, 24, 72)).to(device).float()\n",
    "        te_t300 = torch.from_numpy(teX[:, :, :, 1].reshape(1, 12, 24, 72)).to(device).float()\n",
    "        te_ua = torch.from_numpy(teX[:, :, :, 2].reshape(1, 12, 24, 72)).to(device).float()\n",
    "        te_va = torch.from_numpy(teX[:, :, :, 3].reshape(1, 12, 24, 72)).to(device).float()\n",
    "\n",
    "        prY = predict(te_sst, te_t300)\n",
    "        np.save(res_dir + filename, prY)\n",
    "    \n",
    "    # save to an archive:\n",
    "    shutil.make_archive('result', 'zip', res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-sugar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
